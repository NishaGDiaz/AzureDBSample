{"cells":[{"cell_type":"code","source":["# Set up an account access key\n# spark.conf.set(\n#  \"fs.azure.account.key.<storage-account-name>.blob.core.windows.net\",\n#  \"<storage-access-key>\")\n\nspark.conf.set(\n  \"fs.azure.account.key.datablob26.blob.core.windows.net\",\n  \"gqvfsIb7dirMYkhaXjHMuQVqOPPU8yPC0v3mJs8gW7uYw/Cv1ZdJp495P64niUTVBVRAjhOHzYrM90QqJEaCFw==\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Use standard spark and database api to read from storage account\n# dbutils.fs.ls(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\")\ndbutils.fs.ls(\"wasbs://datablob@datablob26.blob.core.windows.net\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# mount a Blob storage container or a folder inside a container\n# dbutils.fs.mount(\n#   source = \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\",\n#   mount_point = \"<mount-point-path>\",\n#   extra_configs = <\"<conf-key>\": \"<conf-value>\">)\n# [note] <mount_point> is a DBFS path and the path must be under /mnt\n\ndbutils.fs.mount(\n  source = \"wasbs://datablob@datablob26.blob.core.windows.net\",\n  mount_point = \"/mnt/datablob\",\n  extra_configs = {\"fs.azure.account.key.datablob26.blob.core.windows.net\": \"gqvfsIb7dirMYkhaXjHMuQVqOPPU8yPC0v3mJs8gW7uYw/Cv1ZdJp495P64niUTVBVRAjhOHzYrM90QqJEaCFw==\"})"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Access files in your container as if they were local files\n# (TEXT) df = spark.read.text(\"/mnt/%s/....\" % <mount-point-path>)\n# (JSON) df = spark.read.json(\"/mnt/%s/....\" % <mount-point-path>)\n\n# df = spark.read.json( \"/mnt/%s/small_radio_json.json\" % \"datablob\" )\n# Unable to read the above file format as it is a raw json\ndf = spark.read.json( \"/mnt/%s/small_radio_json.json\" % \"datablob\" )\n\n# display(df)\ndf.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# unmount if needed\n# dbutils.fs.unmount(\"<mount-point-path>\")\n# dbutils.fs.unmount(\"/mnt/dbdemo01\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["specificColumnsDf = df.select(\"firstname\", \"lastname\", \"gender\", \"location\", \"level\")\nspecificColumnsDf.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["renamedColumnsDF = specificColumnsDf.withColumnRenamed(\"level\", \"subscription_type\")\nrenamedColumnsDF.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Apply some transformations to the data, then use the\n# Data Source API to write the data back to another table in SQL DW.\n\n# [note] the SQL date warehouse connector uses Azure Blob Storage as a temporary storage to upload data between Azure Databricks and Azure SQL Data Warehouse.\n\n## SQL Data Warehouse related settings\ndwTable= \"mytable001\"\ndwDatabase = \"mysqldb\"\ndwServer = \"rubiks\" \ndwUser = \"nisha\"\ndwPass = \"RithRoh26$\"\ndwJdbcPort =  \"1433\"\ndwJdbcExtraOptions = \"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\nsqlDwUrl = \"jdbc:sqlserver://\" + dwServer + \".database.windows.net:\" + dwJdbcPort + \";database=\" + dwDatabase + \";user=\" + dwUser+\";password=\" + dwPass + \";$dwJdbcExtraOptions\"\nsqlDwUrlSmall = \"jdbc:sqlserver://\" + dwServer + \".database.windows.net:\" + dwJdbcPort + \";database=\" + dwDatabase + \";user=\" + dwUser+\";password=\" + dwPass\n\n\ntempDir = \"wasbs://datablobtmp@datablob26.blob.core.windows.net/tempDirs\"\n\n#sc._jsc.hadoopConfiguration().set(\n#  \"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\",\n#  \"<your-storage-account-access-key>\")\nacntInfo = \"fs.azure.account.key.datablob26.blob.core.windows.net\"\nsc._jsc.hadoopConfiguration().set(\n  acntInfo, \n  \"gqvfsIb7dirMYkhaXjHMuQVqOPPU8yPC0v3mJs8gW7uYw/Cv1ZdJp495P64niUTVBVRAjhOHzYrM90QqJEaCFw==\")\n\n## Loading transformed dataframe (renamedColumnsDF) into SQLDW\nspark.conf.set(\"spark.sql.parquet.writeLegacyFormat\",\"true\")\n\n#df = spark.read.jdbc(url=jdbcUrl, table=dwTable)\n#display(df)\n\n\n## This snippet creates a table called 'dwTable' in the SQL database.\n#df.write \\\n#  .format(\"com.databricks.spark.sqldw\") \\\n#  .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n#  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n#  .option(\"dbtable\", \"my_table_in_dw_copy\") \\\n#  .option(\"tempdir\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\") \\\n#  .save()\n\nrenamedColumnsDF.write \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", sqlDwUrlSmall) \\\n  .option(\"dbtable\", dwTable) \\\n  .option( \"forward_spark_azure_storage_credentials\",\"true\") \\\n  .option(\"tempdir\", tempDir) \\\n  .mode(\"overwrite\") \\\n  .save()"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"blobAzuredb","notebookId":2931145785257610},"nbformat":4,"nbformat_minor":0}
